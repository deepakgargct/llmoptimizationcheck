# Full app.py code is lengthy, so Iâ€™ll provide a Gist or downloadable version.
# Here's a link you can use to download the full app.py:

import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import nltk
import re
import tiktoken
import json

nltk.download('punkt')
nltk.download('stopwords')

# Constants
DANDELION_TOKEN = st.secrets["DANDELION_API_KEY"]
tokenizer = tiktoken.get_encoding("cl100k_base")
stop_words = set(stopwords.words("english"))
EXCLUDED_TAGS = ['header', 'footer', 'nav', 'aside']

AMBIGUOUS_PHRASES = {
    "click here": "Consider using descriptive anchor text like 'View pricing plans'.",
    "learn more": "Be more specific, e.g., 'Learn about AI SEO strategy'.",
    "read more": "Try 'Read the full case study' or 'Explore benefits'.",
    "overview": "Use topic-specific phrasing like 'SEO strategy breakdown'.",
    "details": "Say 'Implementation checklist' or 'Feature breakdown'.",
    "more info": "Specify what info is available, like 'FAQ on billing'.",
    "start now": "Clarify with action, e.g., 'Start your free SEO audit now'.",
    "this article": "Use the actual article title for clarity.",
    "some believe": "Attribute to a source or remove vague phrasing.",
    "it could be argued": "Specify who argues or rephrase for clarity."
}

def count_tokens(text):
    return len(tokenizer.encode(text))

def analyze_readability(text):
    try:
        import textstat
        return textstat.flesch_reading_ease(text)
    except:
        return 0.0

def fetch_url_content(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=10)
        if 'text/html' not in res.headers.get('Content-Type', ''):
            return None
        return BeautifulSoup(res.text, 'html.parser')
    except:
        return None

def remove_duplicates(chunks):
    seen = set()
    filtered = []
    for c in chunks:
        if c['text'] not in seen:
            filtered.append(c)
            seen.add(c['text'])
    return filtered

def extract_chunks(soup):
    for tag in EXCLUDED_TAGS:
        for elem in soup.find_all(tag):
            elem.decompose()

    chunks = []
    for tag in soup.find_all(['section', 'article', 'div', 'p']):
        text = tag.get_text(strip=True)
        if len(text.split()) > 30:
            tokens = count_tokens(text)
            readability = analyze_readability(text)
            ambiguous = [phrase for phrase in AMBIGUOUS_PHRASES if phrase in text.lower()]
            suggestions = [fâš ï¸ Ambiguous: '{p}' â†’ {AMBIGUOUS_PHRASES[p]}" for p in ambiguous]

            score = 100
            if tokens > 300: score -= 10
            if readability < 60: score -= 10
            if ambiguous: score -= 10 * len(ambiguous)

            chunks.append({
                'text': text[:400] + '...' if len(text) > 400 else text,
                'token_count': tokens,
                'readability': readability,
                'ambiguous_phrases': ambiguous,
                'quality_score': max(score, 0),
                'suggestions': suggestions
            })
    return remove_duplicates(chunks)

def extract_entities_dandelion(text):
    try:
        url = "https://api.dandelion.eu/datatxt/nex/v1/"
        payload = {
            'text': text,
            'lang': 'en',
            'token': DANDELION_TOKEN
        }
        response = requests.post(url, data=payload).json()
        return [e['spot'] for e in response.get("annotations", [])]
    except:
        return []

def extract_glossary(chunks):
    glossary = []
    pattern = re.compile(r"(?P<term>[A-Z][a-zA-Z0-9\- ]+?)\s+(is|means|refers to)\s+(?P<definition>.+?)\.")
    for c in chunks:
        matches = pattern.findall(c['text'])
        for m in matches:
            glossary.append({"term": m[0].strip(), "definition": m[2].strip()})
    return glossary

def get_key_takeaways(chunks, top=3):
    return sorted(chunks, key=lambda x: x['quality_score'], reverse=True)[:top]

def generate_ai_tip(chunk):
    tip = []
    if chunk['readability'] < 60:
        tip.append("ğŸ’¡ Improve sentence clarity and shorten paragraphs.")
    if chunk['token_count'] > 300:
        tip.append("ğŸ’¡ Split into smaller, focused sections (~200 tokens).")
    if chunk['ambiguous_phrases']:
        tip.append("ğŸ’¡ Replace vague terms with topic-specific language.")
    return " ".join(tip)

def suggest_schema_types(text):
    lower = text.lower()
    if "event" in lower: return "Event schema"
    elif "review" in lower: return "Review schema"
    elif "product" in lower: return "Product schema"
    elif "faq" in lower: return "FAQPage schema"
    return "Article schema"

def dummy_brave_perplexity_check(text):
    return "Likely Appears âœ…" if "seo" in text.lower() else "Not Found âŒ"

# Streamlit UI
st.set_page_config(page_title="GenAI Optimization Checker", layout="wide")
st.title("GenAI Optimization Checker")

with st.sidebar:
    st.markdown("### ğŸ§¾ Content Input")
    method = st.radio("Choose input method", ["URL", "HTML file", "Raw HTML", ".txt file"])
    html = ""

    if method == "URL":
        url = st.text_input("Enter URL:")
        if st.button("Fetch"):
            soup = fetch_url_content(url)
            html = soup.prettify() if soup else ""
    elif method == "HTML file":
        f = st.file_uploader("Upload HTML", type="html")
        if f: html = f.read().decode("utf-8")
    elif method == "Raw HTML":
        html = st.text_area("Paste HTML:")
    elif method == ".txt file":
        f = st.file_uploader("Upload .txt", type="txt")
        if f: html = f.read().decode("utf-8")

if html:
    soup = BeautifulSoup(html, "html.parser")
    chunks = extract_chunks(soup)

    tabs = st.tabs(["ğŸ§  AI Analysis", "ğŸ“š Glossary", "ğŸ“Œ Takeaways", "ğŸ“Š Visuals", "ğŸ“¤ Export"])

    with tabs[0]:
        for i, c in enumerate(chunks):
            st.markdown(f"### Chunk {i+1}")
            st.markdown(c['text'])
            st.markdown(f"**Tokens:** {c['token_count']} | **Readability:** {c['readability']:.2f} | **Score:** {c['quality_score']}%")
            for s in c['suggestions']:
                st.warning(s)
            ai_tip = generate_ai_tip(c)
            if ai_tip: st.info(f"ğŸ§  LLM Tip: {ai_tip}")
            schema_type = suggest_schema_types(c['text'])
            st.markdown(f"ğŸ“˜ Suggested Schema: **{schema_type}**")
            visibility = dummy_brave_perplexity_check(c['text'])
            st.markdown(f"ğŸ” AI Search Visibility: **{visibility}**")
            entities = extract_entities_dandelion(c['text'])
            if entities:
                st.markdown(f"ğŸ§¾ Entities: {', '.join(set(entities))}")

    with tabs[1]:
        st.subheader("ğŸ“š Glossary")
        glossary = extract_glossary(chunks)
        if glossary:
            df = pd.DataFrame(glossary)
            st.dataframe(df)
        else:
            st.info("No glossary entries found.")

    with tabs[2]:
        st.subheader("ğŸ“Œ Key Takeaways")
        for i, c in enumerate(get_key_takeaways(chunks)):
            st.markdown(f"**Takeaway {i+1}:** {c['text']}")

    with tabs[3]:
        df = pd.DataFrame(chunks)
        st.subheader("ğŸ“Š Score Distribution")
        fig1, ax1 = plt.subplots()
        sns.histplot(df['quality_score'], ax=ax1, color='skyblue')
        st.pyplot(fig1)

        st.subheader("ğŸ“Š Token Count vs Readability")
        fig2, ax2 = plt.subplots()
        sns.scatterplot(data=df, x="token_count", y="readability", ax=ax2)
        st.pyplot(fig2)

    with tabs[4]:
        flagged = df[df['quality_score'] < 70]
        if not flagged.empty:
            st.dataframe(flagged)
            csv = flagged.to_csv(index=False).encode()
            st.download_button("ğŸ“¥ Download CSV", csv, "flagged_chunks.csv", "text/csv")
        else:
            st.success("No low-scoring chunks found!")

st.caption("âš¡ Optimized for AI search engines and LLM retrieval readiness.")
